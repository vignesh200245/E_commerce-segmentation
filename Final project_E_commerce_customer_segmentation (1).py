# -*- coding: utf-8 -*-
"""Project 3-E-commerce Customer Segementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/rreghunandan/Project--E-Commerce-Customer-Segmentation/blob/main/Project_3_E_commerce_Customer_Segementation.ipynb

Project On E -Commerce Customer Segmentation

The dataset contains details regarding the Invoice number,Stock code,description,Qunatity,Invoice date,Unit Price,CustomerID & Country

# Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import datetime, nltk, warnings

"""# Load and read the dataset"""

df=pd.read_csv("/content/data.csv",encoding="ISO-8859-1")
df.shape

df.head()         # displays five rows

df.dtypes                                                                     # data type

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
#df['InvoiceNo']=df['InvoiceNo'].astype(int)
#df["CustomerID"]=df["CustomerID"].astype(int)
df.dtypes

df.isnull().sum()                                                    # check for null values

"""With the enormous data available, it is impossible to impute values for the customerID. So deleting the missing values in customerID from the dataframe."""

df.dropna(axis = 0, subset = ['CustomerID'], inplace = True)                              #dropping the missing values/rows from the CustomerID
df.shape

df.isnull().sum()

"""#Checking for duplicate entries and deleting them ."""

df['Country'].duplicated().value_counts()

df['InvoiceNo'].duplicated().value_counts()

df.duplicated().sum()

df.drop_duplicates(inplace = True)
df.shape

"""# Doing NLP in the description colum
-removing spaces & number
-converting to small letters
"""

df["Description"]

df["Desc_new"]=df["Description"].str.replace("[^a-zA-Z]","  ")
df["Desc_new"]=df["Desc_new"].astype(str)
df["Desc_new"]

df["Desc_new"]=df["Desc_new"].apply(lambda row:" ".join([word for word in row.split() if len(word)>2]))
df["Desc_new"]

df["Desc_new"]=[review.lower() for review in df["Desc_new"]]
df["Desc_new"]

"""#Removing Stop words ,lemmatising"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk import word_tokenize
stopwrds =stopwords.words('english')

#Making cutom list of stop words to be removed
addwrds=[]

#Adding to thelist of words
stopwrds.extend(addwrds)

#Function to remove stop words
def remove_stopwrds(rev):
  review_tokenized=word_tokenize(rev)
  rev_new=" ".join([i for i in review_tokenized if i not in stopwrds])
  return rev_new


#Removing Stop words
df['Desc_new']=[remove_stopwrds(r) for r in df['Desc_new']]

#Begin Lemmatisation
nltk.download("wordnet")
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import wordnet
nltk.download('averaged_perceptron_tagger')

#function to convert nltk tag to wordnet tag
lemmatizer=WordNetLemmatizer()

def nltk_tag_to_wordnet_tag(nltk_tag):
  if nltk_tag.startwith('J'):
    return wordnet.ADJ
  elif nltk_tag.startwith('V'):
    return wordnet.VERB
  elif nltk_tag.startwith('N'):
    return wordnet.NOUN
  elif nltk_tag.startwith('R'):
    return wordnet.ADJ
  else:
    return None


def lemmatize_sentence(sentence):
  nltk_tagged = nltk.pos_tag(nltk.word_-word_tokenize(sentence))
  wordnet_tagged=map(lambda x:(x[0],nltk_tag_to_wordnet_tag(x[1])),nltk_tagged)

  lemmatized_sentence = []
  for word,tag in wordnet_tagged:
    if tag is None:
      lemmatized_sentence.append(word)
    else:
     lemmatized_sentence.append(lemmatizer.lemmatize(word,tag))
  return " ".join(lemmatized_sentence)


  df["Desc_new"]=df['Desc_new'].apply(lambda x:lemmatize_sentence(x))

df

#Removing duplicated descriptions
df2=df["Desc_new"].drop_duplicates()
df2=pd.DataFrame(df2)
df2

#Import the libraries we need
from sklearn.feature_extraction.text import CountVectorizer # give you the bag of words model
import pandas as pd

# Step 2. Design the Vocabulary
# The default token pattern removes tokens of a single character. That's why we don't have the "I" and "s" tokens in the output
count_vectorizer = CountVectorizer(binary = False)

# Step 3. Create the Bag-of-Words Model
bag_of_words = count_vectorizer.fit_transform(df2["Desc_new"]) # fit - design the vocbulary and transform will convert the text into numbers based on the presence of the word

# Show the Bag-of-Words Model as a pandas DataFrame
feature_names = count_vectorizer.get_feature_names()
df3=pd.DataFrame(bag_of_words.toarray(), columns = feature_names)

#shows the words in description colum
df3

x=bag_of_words.toarray()
x

"""#Using K-means clustering"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# %matplotlib inline
plt.style.use("fivethirtyeight")

from warnings import filterwarnings
filterwarnings("ignore")

"""To find the optimal value of K we are use Elbow plot ,where k is the hyperparameter.

"""

list_k=list(range(1,25))
inertias = []
for k in list_k:
    km = KMeans(n_clusters=k)
    km.fit(x)
    inertias.append(km.inertia_)

# Make elbow plot
plt.figure(figsize = (5, 5))
plt.title('Elbow plot')
plt.plot(list_k,inertias,'-o')
plt.xlabel('Clusters of *k*')
plt.ylabel('Sum of squared error')

"""Here we used Kmeans to find the optimal vlaue of K Since there is no optimal value that can be infered here with the elbow plot we will do PCA.(Principal Component Analysis)"""

sc=StandardScaler()                                                                 #performing standarisation
X_scaled= sc.fit_transform(x)

from sklearn.decomposition import PCA
components= None
pca=PCA(n_components = components)                                        #components is the number of reduced relevant columns (hyperparameter)
pca.fit(x)

#prints the  variance
print("Variances(Percentage)")
print(pca.explained_variance_ratio_*100)

print ("cumulative variance (Percentage)")
print((pca.explained_variance_ratio_.cumsum() * 100)[:100])

#plot the cumulative varience
components=len(pca.explained_variance_ratio_)\
  if components  is None else components
plt.plot(range(1,components+1),
         np.cumsum(pca.explained_variance_ratio_*100))

plt.xlabel("Number of components")
plt.ylabel("Explained variance(%)")

#From the above graph we can see that it takes 1500 components to reach flat curve(100% varience).
# choosing around  90% of variations:
from sklearn.decomposition import PCA

pca = PCA(n_components = 0.90)
pca.fit(x)

# optimum no:of components
components = len(pca.explained_variance_ratio_)
print(f'Number of components: {components}')

# Make the scree plot
plt.plot(range(1, components + 1), np.cumsum(pca.explained_variance_ratio_ * 100))
plt.xlabel("Number of components")
plt.ylabel("Explained variance (%)")

"""Applying PCA:
PCA  helps in  lower dimension of data,while keeping all original variables in the model.
"""

from sklearn.decomposition import PCA

pca=PCA(638)
Principal_Component_Analysis=pca.fit_transform(x)
Principal_Component_Analysis.shape

#Elbow plot
#We make a plot btwn K value and inertia

list_k=list(range(1,20))
inertias = []
for k in list_k:
    km = KMeans(n_clusters=k)
    km.fit(x)
    inertias.append(km.inertia_)

# Make elbow plot
plt.figure(figsize = (5, 5))
plt.title('Elbow plot')
plt.plot(list_k,inertias,'-o')
plt.xlabel('Clusters of *k*')
plt.ylabel('Sum of squared error')

#km modelling
km=KMeans(n_clusters=12)         #applying k
km.fit(Principal_Component_Analysis )   #fit the data

centroids = km.cluster_centers_

#shows which group each datapoint belongs to
km.labels_

#Predicts the labels of cluster
label=km.fit_predict(Principal_Component_Analysis)
print(label)

#Getting the centroids
centroids=km.cluster_centers_
llabels=np.unique(label)

#plotting

plt.figure(figsize=(10,6))
for i in llabels:
  plt.scatter(Principal_Component_Analysis[label == i,0],Principal_Component_Analysis[label == i,1],label =i)
plt.scatter(centroids[:,0],centroids[:,1],s=100,c="k",label="centroids")
plt.legend()
plt.show()

"""#Concatenating the label,NLP description  into original dataframe"""

df5=pd.DataFrame(km.labels_)
print(df5.shape)
df5

df2=df2.reset_index(drop=True)
df2

df6=df2.join(df5)
df6.rename(columns={0:'Product Code'},inplace=True)
df6.head(3)

df8=pd.merge(df,df6,how="left",on="Desc_new")
df8

df9 = pd.get_dummies(df8,columns=["Product Code"])     ##product code one hot encoding
df9.head()

df10 = df9.copy()      #copying df9

df10 = df10.drop(["InvoiceNo","StockCode","Description","InvoiceDate","Country","Desc_new"],axis=1)
df10.head()

"""Grouping the Customers based on CustomerID:"""

df11 = df10.groupby(['CustomerID']).mean()
df11

df11.describe()

# Converting into numpy array:
y = df11.to_numpy()
y

from sklearn.preprocessing import StandardScaler, MinMaxScaler
sc = MinMaxScaler()
y_scaled = sc.fit_transform(y)

list_k=list(range(1,15))
inertias = []
for k in list_k:
    km = KMeans(n_clusters=k)
    km.fit(y_scaled)
    inertias.append(km.inertia_)

plt.figure(figsize = (5, 5))
plt.title('Elbow plot')
plt.plot(list_k,inertias,'-o')
plt.xlabel('Clusters of *k*')
plt.ylabel('Sum of squared error')

"""From Elbow plot we can say that the optimal  K value is at 4,thus the customers can be clusterd into 4 clusters based on their similarities"""

km = KMeans(n_clusters=4)     # applying k = 4
km.fit(y_scaled)          # fit the data

centroids = km.cluster_centers_   # final centroid points

# print("centroids: ",centroids)
print("inertia: ",km.inertia_)

km.labels_

label = km.fit_predict(y_scaled)
print(label)

"""Visualizing the customer clusters:"""

# Getting the Centroids and Cluster labels
centroids = km.cluster_centers_
labels = np.unique(label)

#  plotting
plt.figure(figsize=(5, 5))
for i in labels:
    plt.scatter(y_scaled[label == i , 0] , y_scaled[label == i , 1] , label = i)
plt.scatter(centroids[:,0] , centroids[:,1] ,  c="k", s=150, label="centroids")
plt.legend()
plt.show()

"""Since the graph doesnt give a clear picture of the clusters formed ,we are converting the label to dataframe and assigne the respective labels to their clusters and plot them."""

df13 = pd.DataFrame(label)
df13.head()

df13.reset_index(level=0, inplace=True)                  #reseting  the index
df13.head()

df13.rename(columns={'index':'ID', 0:'Customer cluster'},inplace=True)
df13.head()

df14 = df13.groupby("Customer cluster").count()           #grouping the df based n the custmoer cluster and counting  customers belonging to each cluster
df14

import seaborn as sns
sns.countplot(df13["Customer cluster"])

"""From the  above graph Customer Cluster 2 is having the maximum number of Cutomers and cluster 0 has the minimum number of customers"""